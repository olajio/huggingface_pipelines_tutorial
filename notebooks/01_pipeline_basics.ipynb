{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Hugging Face Pipeline Basics\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "- What a pipeline is and why it exists\n",
    "- The internal workflow of a pipeline\n",
    "- How to create and configure pipelines\n",
    "- Key parameters and options\n",
    "- Device management (CPU/GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed (uncomment)\n",
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is a Pipeline?\n",
    "\n",
    "A **Pipeline** is a high-level API that abstracts away the complexity of:\n",
    "1. Loading the correct model and tokenizer\n",
    "2. Preprocessing (tokenization, tensor conversion)\n",
    "3. Running inference\n",
    "4. Post-processing (decoding, formatting)\n",
    "\n",
    "### The Pipeline Workflow\n",
    "\n",
    "```\n",
    "Raw Input â†’ Tokenizer â†’ Model â†’ Post-processor â†’ Output\n",
    "   â†“            â†“          â†“           â†“            â†“\n",
    "\"Hello\"    [101,7592]   logits     softmax    POSITIVE 99%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Your First Pipeline\n",
    "\n",
    "The simplest way to create a pipeline is to specify just the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sentiment analysis pipeline\n",
    "# This automatically downloads the default model for this task\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Use it!\n",
    "result = classifier(\"I love learning about transformers and NLP!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Output\n",
    "\n",
    "The output is a list of dictionaries. Each dictionary contains:\n",
    "- `label`: The predicted class (e.g., POSITIVE, NEGATIVE)\n",
    "- `score`: Confidence score (0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the output structure\n",
    "for item in result:\n",
    "    print(f\"Label: {item['label']}\")\n",
    "    print(f\"Score: {item['score']:.4f}\")\n",
    "    print(f\"Confidence: {item['score']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What's Happening Under the Hood?\n",
    "\n",
    "Let's break down what the pipeline does internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT a pipeline - doing it manually:\n",
    "\n",
    "# Step 1: Load tokenizer and model\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Step 2: Tokenize input\n",
    "text = \"I love learning about transformers!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "print(\"Tokenized inputs:\")\n",
    "print(f\"Input IDs: {inputs['input_ids']}\")\n",
    "print(f\"Attention Mask: {inputs['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Run model inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "print(f\"Raw logits: {logits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Post-process (softmax to get probabilities)\n",
    "probabilities = torch.softmax(logits, dim=-1)\n",
    "predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "confidence = probabilities[0][predicted_class].item()\n",
    "\n",
    "# Map to label\n",
    "labels = model.config.id2label\n",
    "print(f\"\\nPredicted: {labels[predicted_class]} ({confidence:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"That was ~15 lines of code. With pipeline: 2 lines!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline Configuration Options\n",
    "\n",
    "Pipelines are highly configurable. Here are the key parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Specify a different model\n",
    "classifier_v2 = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\"  # 5-star rating model\n",
    ")\n",
    "\n",
    "result = classifier_v2(\"This product is absolutely fantastic!\")\n",
    "print(f\"5-star model result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Specify model AND tokenizer separately\n",
    "classifier_custom = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    tokenizer=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "# This is useful when you have custom tokenizers or fine-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3: Use pre-loaded model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "# Pass them to pipeline\n",
    "classifier_preloaded = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(classifier_preloaded(\"This is really useful for production!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Device Management (CPU/GPU)\n",
    "\n",
    "Pipelines can run on CPU or GPU. Here's how to control this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Explicit device ID\n",
    "# device=0 means first GPU, device=-1 means CPU\n",
    "if torch.cuda.is_available():\n",
    "    classifier_gpu = pipeline(\"sentiment-analysis\", device=0)\n",
    "    print(\"Running on GPU!\")\n",
    "else:\n",
    "    classifier_cpu = pipeline(\"sentiment-analysis\", device=-1)\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Automatic device mapping (great for large models)\n",
    "# device_map=\"auto\" automatically distributes model across available devices\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2\",\n",
    "    device_map=\"auto\"  # Automatically chooses best device\n",
    ")\n",
    "\n",
    "print(\"Model loaded with automatic device mapping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Processing\n",
    "\n",
    "Pipelines can efficiently process multiple inputs at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single input\n",
    "single_result = classifier(\"I love this!\")\n",
    "\n",
    "# Batch input - just pass a list!\n",
    "texts = [\n",
    "    \"I absolutely love this product!\",\n",
    "    \"This is terrible, worst purchase ever.\",\n",
    "    \"It's okay, nothing special.\",\n",
    "    \"Amazing quality, highly recommended!\",\n",
    "    \"Not worth the money.\"\n",
    "]\n",
    "\n",
    "batch_results = classifier(texts)\n",
    "\n",
    "# Display results\n",
    "for text, result in zip(texts, batch_results):\n",
    "    print(f\"{result['label']:8} ({result['score']:.2f}): {text[:40]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control batch size for memory management\n",
    "large_batch = texts * 100  # 500 texts\n",
    "\n",
    "# Process with explicit batch size\n",
    "results = classifier(large_batch, batch_size=16)  # Process 16 at a time\n",
    "\n",
    "print(f\"Processed {len(results)} texts\")\n",
    "print(f\"Sample result: {results[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pipeline Parameters at Inference Time\n",
    "\n",
    "Many parameters can be set when calling the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with text generation - many inference parameters\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Basic generation\n",
    "result = generator(\"The future of AI is\")\n",
    "print(\"Basic:\", result[0]['generated_text'])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With parameters\n",
    "result = generator(\n",
    "    \"The future of AI is\",\n",
    "    max_length=50,           # Maximum length of generated text\n",
    "    num_return_sequences=3,  # Generate 3 different completions\n",
    "    temperature=0.7,         # Control randomness (lower = more deterministic)\n",
    "    top_k=50,               # Only sample from top 50 tokens\n",
    "    top_p=0.95,             # Nucleus sampling\n",
    "    do_sample=True          # Enable sampling (vs greedy decoding)\n",
    ")\n",
    "\n",
    "print(\"Generated sequences:\")\n",
    "for i, seq in enumerate(result):\n",
    "    print(f\"\\n{i+1}. {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Accessing Pipeline Components\n",
    "\n",
    "You can access the underlying model and tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Access the model\n",
    "print(f\"Model: {type(classifier.model).__name__}\")\n",
    "print(f\"Model config: {classifier.model.config.model_type}\")\n",
    "\n",
    "# Access the tokenizer\n",
    "print(f\"\\nTokenizer: {type(classifier.tokenizer).__name__}\")\n",
    "print(f\"Vocab size: {classifier.tokenizer.vocab_size}\")\n",
    "\n",
    "# Access device info\n",
    "print(f\"\\nDevice: {classifier.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can even use the tokenizer directly\n",
    "tokens = classifier.tokenizer(\"Hello world!\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Decode back\n",
    "decoded = classifier.tokenizer.decode(tokens['input_ids'])\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Memory Management and Model Loading\n",
    "\n",
    "Tips for managing memory when working with pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Tip 1: Use half precision to reduce memory\n",
    "if torch.cuda.is_available():\n",
    "    classifier_fp16 = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        torch_dtype=torch.float16,  # Half precision\n",
    "        device=0\n",
    "    )\n",
    "    print(\"Loaded model in FP16 (half precision)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip 2: Delete pipeline and free memory when done\n",
    "del classifier\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Memory freed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip 3: Use smaller models for development\n",
    "# DistilBERT is ~40% smaller than BERT but retains 97% performance\n",
    "small_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"distilbert-base-uncased\"  # Smaller model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary: Pipeline Creation Patterns\n",
    "\n",
    "Here's a reference of the different ways to create pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Task only (uses default model)\n",
    "pipe1 = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Pattern 2: Task + specific model\n",
    "pipe2 = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Pattern 3: Task + model + tokenizer\n",
    "pipe3 = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    tokenizer=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "# Pattern 4: Full configuration\n",
    "pipe4 = pipeline(\n",
    "    task=\"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    tokenizer=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(\"All patterns work! Choose based on your needs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "1. **Pipelines are abstractions** - They wrap tokenizer + model + post-processing\n",
    "2. **Minimal code** - 2-3 lines vs 15-20 lines manually\n",
    "3. **Configurable** - Choose model, device, precision, batch size\n",
    "4. **Automatic downloads** - Models come from Hugging Face Hub\n",
    "5. **Batch processing** - Pass a list for efficient processing\n",
    "6. **Access internals** - `.model` and `.tokenizer` attributes available\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to [02_nlp_pipelines.ipynb](02_nlp_pipelines.ipynb) to explore all NLP pipeline types!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
