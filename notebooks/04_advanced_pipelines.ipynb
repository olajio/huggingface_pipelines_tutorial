{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Advanced Pipelines\n",
    "\n",
    "This notebook covers advanced topics:\n",
    "\n",
    "1. Creating Custom Pipelines\n",
    "2. Pipeline Optimization (Quantization, ONNX)\n",
    "3. Using Local Models\n",
    "4. Pipeline Callbacks and Hooks\n",
    "5. Combining Multiple Pipelines\n",
    "6. Error Handling and Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    pipeline, \n",
    "    Pipeline,\n",
    "    AutoTokenizer, \n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig\n",
    ")\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Creating Custom Pipelines\n",
    "\n",
    "Sometimes you need functionality not covered by built-in pipelines.\n",
    "\n",
    "### 1.1 Custom Pipeline from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pipeline\n",
    "\n",
    "class SimilarityPipeline(Pipeline):\n",
    "    \"\"\"\n",
    "    Custom pipeline to compute text similarity scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        \"\"\"Handle pipeline parameters.\"\"\"\n",
    "        preprocess_kwargs = {}\n",
    "        postprocess_kwargs = {}\n",
    "        \n",
    "        if \"text_pair\" in kwargs:\n",
    "            preprocess_kwargs[\"text_pair\"] = kwargs[\"text_pair\"]\n",
    "        \n",
    "        return preprocess_kwargs, {}, postprocess_kwargs\n",
    "    \n",
    "    def preprocess(self, text, text_pair=None):\n",
    "        \"\"\"Tokenize the input texts.\"\"\"\n",
    "        if text_pair is None:\n",
    "            raise ValueError(\"text_pair is required for similarity\")\n",
    "        \n",
    "        # Tokenize both texts\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            text_pair,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        return inputs\n",
    "    \n",
    "    def _forward(self, model_inputs):\n",
    "        \"\"\"Run the model.\"\"\"\n",
    "        outputs = self.model(**model_inputs)\n",
    "        return outputs\n",
    "    \n",
    "    def postprocess(self, model_outputs):\n",
    "        \"\"\"Process model outputs into similarity scores.\"\"\"\n",
    "        # Get the [CLS] token embedding\n",
    "        logits = model_outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Assume binary classification (similar/not similar)\n",
    "        similarity_score = probabilities[0][1].item()\n",
    "        \n",
    "        return {\n",
    "            \"similarity\": similarity_score,\n",
    "            \"label\": \"similar\" if similarity_score > 0.5 else \"not similar\"\n",
    "        }\n",
    "\n",
    "print(\"Custom SimilarityPipeline class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the custom pipeline\n",
    "# Note: This requires an appropriate model trained for similarity\n",
    "\n",
    "# For demonstration, we'll use a paraphrase model\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"textattack/bert-base-uncased-MRPC\"  # Paraphrase detection model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create custom pipeline\n",
    "similarity_pipe = SimilarityPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1  # CPU\n",
    ")\n",
    "\n",
    "# Test it\n",
    "result = similarity_pipe(\n",
    "    \"The cat sat on the mat.\",\n",
    "    text_pair=\"A cat was sitting on a mat.\"\n",
    ")\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Extending Existing Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "class EnhancedClassificationPipeline(TextClassificationPipeline):\n",
    "    \"\"\"\n",
    "    Extended classification pipeline with additional features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def postprocess(self, model_outputs, function_to_apply=None, top_k=1, _legacy=True):\n",
    "        # Call parent's postprocess\n",
    "        results = super().postprocess(\n",
    "            model_outputs, \n",
    "            function_to_apply=function_to_apply,\n",
    "            top_k=top_k,\n",
    "            _legacy=_legacy\n",
    "        )\n",
    "        \n",
    "        # Add custom fields\n",
    "        for result in results:\n",
    "            # Add confidence level\n",
    "            score = result['score']\n",
    "            if score > 0.9:\n",
    "                result['confidence'] = 'high'\n",
    "            elif score > 0.7:\n",
    "                result['confidence'] = 'medium'\n",
    "            else:\n",
    "                result['confidence'] = 'low'\n",
    "            \n",
    "            # Add emoji based on sentiment\n",
    "            if 'POSITIVE' in result['label'].upper():\n",
    "                result['emoji'] = 'ðŸ˜Š'\n",
    "            elif 'NEGATIVE' in result['label'].upper():\n",
    "                result['emoji'] = 'ðŸ˜ž'\n",
    "            else:\n",
    "                result['emoji'] = 'ðŸ˜'\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Use the enhanced pipeline\n",
    "enhanced_classifier = EnhancedClassificationPipeline(\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "result = enhanced_classifier(\"I absolutely love this!\")\n",
    "print(f\"Enhanced result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Pipeline Optimization\n",
    "\n",
    "### 2.1 Using Half Precision (FP16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP16 reduces memory by ~50% with minimal accuracy loss\n",
    "if torch.cuda.is_available():\n",
    "    classifier_fp16 = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device=0\n",
    "    )\n",
    "    print(\"FP16 pipeline created on GPU\")\n",
    "else:\n",
    "    print(\"GPU not available - FP16 works best on GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Quantization (INT8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic quantization for CPU inference\n",
    "# This reduces model size and speeds up CPU inference\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Apply dynamic quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    {torch.nn.Linear},  # Quantize linear layers\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Create pipeline with quantized model\n",
    "quantized_classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=quantized_model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "result = quantized_classifier(\"This is great!\")\n",
    "print(f\"Quantized model result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Using ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNX provides optimized inference\n",
    "# Install: pip install optimum[onnxruntime]\n",
    "\n",
    "try:\n",
    "    from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "    from transformers import AutoTokenizer, pipeline\n",
    "    \n",
    "    # Load model in ONNX format\n",
    "    model = ORTModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        export=True  # Convert to ONNX\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    \n",
    "    # Create ONNX-optimized pipeline\n",
    "    onnx_classifier = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    result = onnx_classifier(\"ONNX is fast!\")\n",
    "    print(f\"ONNX result: {result}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Install optimum: pip install optimum[onnxruntime]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Batching for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Generate test data\n",
    "texts = [f\"This is test sentence number {i}.\" for i in range(100)]\n",
    "\n",
    "# Method 1: One at a time (slow)\n",
    "start = time.time()\n",
    "results_slow = [classifier(t) for t in texts]\n",
    "time_slow = time.time() - start\n",
    "print(f\"One at a time: {time_slow:.2f}s\")\n",
    "\n",
    "# Method 2: Batch processing (faster)\n",
    "start = time.time()\n",
    "results_batch = classifier(texts, batch_size=16)\n",
    "time_batch = time.time() - start\n",
    "print(f\"Batch (size=16): {time_batch:.2f}s\")\n",
    "\n",
    "print(f\"Speedup: {time_slow/time_batch:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Using Local Models\n",
    "\n",
    "### 3.1 Save and Load Models Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load model from Hub\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Save locally\n",
    "save_dir = \"./local_model\"\n",
    "classifier.model.save_pretrained(save_dir)\n",
    "classifier.tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"Model saved to {save_dir}\")\n",
    "print(f\"Files: {os.listdir(save_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from local directory\n",
    "local_classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=save_dir,\n",
    "    tokenizer=save_dir\n",
    ")\n",
    "\n",
    "result = local_classifier(\"Testing local model!\")\n",
    "print(f\"Local model result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Offline Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For air-gapped environments\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\n",
    "\n",
    "# Now the library won't try to download anything\n",
    "# It will only use locally cached models\n",
    "\n",
    "# Reset for this notebook\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"\n",
    "print(\"Offline mode disabled for this notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Combining Multiple Pipelines\n",
    "\n",
    "### 4.1 Sequential Pipeline Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineChain:\n",
    "    \"\"\"\n",
    "    Chain multiple pipelines together.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.ner = pipeline(\"ner\", grouped_entities=True)\n",
    "        self.sentiment = pipeline(\"sentiment-analysis\")\n",
    "        self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    \n",
    "    def analyze(self, text):\n",
    "        \"\"\"Run comprehensive text analysis.\"\"\"\n",
    "        results = {\n",
    "            'original_text': text,\n",
    "            'char_count': len(text),\n",
    "            'word_count': len(text.split())\n",
    "        }\n",
    "        \n",
    "        # 1. Extract entities\n",
    "        entities = self.ner(text)\n",
    "        results['entities'] = [\n",
    "            {'text': e['word'], 'type': e['entity_group'], 'score': round(e['score'], 3)}\n",
    "            for e in entities\n",
    "        ]\n",
    "        \n",
    "        # 2. Sentiment\n",
    "        sentiment = self.sentiment(text)[0]\n",
    "        results['sentiment'] = {\n",
    "            'label': sentiment['label'],\n",
    "            'score': round(sentiment['score'], 3)\n",
    "        }\n",
    "        \n",
    "        # 3. Summary (if text is long enough)\n",
    "        if len(text.split()) > 50:\n",
    "            summary = self.summarizer(text, max_length=50, min_length=20)\n",
    "            results['summary'] = summary[0]['summary_text']\n",
    "        else:\n",
    "            results['summary'] = 'Text too short for summarization'\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Usage\n",
    "chain = PipelineChain()\n",
    "\n",
    "text = \"\"\"\n",
    "Apple Inc. reported record quarterly revenue of $123.9 billion today, \n",
    "driven by strong iPhone sales. CEO Tim Cook announced the results from \n",
    "their headquarters in Cupertino, California. The company's stock rose \n",
    "5% in after-hours trading. Analysts from Goldman Sachs and Morgan Stanley \n",
    "praised the results, calling it a \"exceptional performance\" despite \n",
    "global supply chain challenges.\n",
    "\"\"\"\n",
    "\n",
    "analysis = chain.analyze(text)\n",
    "print(\"=\" * 50)\n",
    "print(\"TEXT ANALYSIS REPORT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nWord Count: {analysis['word_count']}\")\n",
    "print(f\"\\nSentiment: {analysis['sentiment']['label']} ({analysis['sentiment']['score']})\")\n",
    "print(f\"\\nEntities Found:\")\n",
    "for e in analysis['entities']:\n",
    "    print(f\"  - {e['type']}: {e['text']}\")\n",
    "print(f\"\\nSummary: {analysis['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Parallel Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "class ParallelPipelines:\n",
    "    \"\"\"\n",
    "    Run multiple pipelines in parallel.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.pipelines = {\n",
    "            'sentiment': pipeline(\"sentiment-analysis\"),\n",
    "            'ner': pipeline(\"ner\", grouped_entities=True),\n",
    "            'zero_shot': pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "        }\n",
    "    \n",
    "    def _run_sentiment(self, text):\n",
    "        return {'sentiment': self.pipelines['sentiment'](text)}\n",
    "    \n",
    "    def _run_ner(self, text):\n",
    "        return {'ner': self.pipelines['ner'](text)}\n",
    "    \n",
    "    def _run_zero_shot(self, text, labels):\n",
    "        return {'topics': self.pipelines['zero_shot'](text, labels)}\n",
    "    \n",
    "    def analyze_parallel(self, text, topic_labels):\n",
    "        \"\"\"Run all analyses in parallel.\"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            futures = [\n",
    "                executor.submit(self._run_sentiment, text),\n",
    "                executor.submit(self._run_ner, text),\n",
    "                executor.submit(self._run_zero_shot, text, topic_labels)\n",
    "            ]\n",
    "            \n",
    "            results = {}\n",
    "            for future in futures:\n",
    "                results.update(future.result())\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Usage\n",
    "parallel = ParallelPipelines()\n",
    "\n",
    "text = \"Microsoft announced a new partnership with OpenAI to develop advanced AI systems.\"\n",
    "labels = [\"technology\", \"business\", \"science\", \"sports\"]\n",
    "\n",
    "start = time.time()\n",
    "results = parallel.analyze_parallel(text, labels)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Parallel analysis completed in {elapsed:.2f}s\")\n",
    "print(f\"\\nSentiment: {results['sentiment']}\")\n",
    "print(f\"Entities: {[e['word'] for e in results['ner']]}\")\n",
    "print(f\"Top topic: {results['topics']['labels'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Error Handling and Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline wrapper with error handling.\n",
    "    \"\"\"\n",
    "    def __init__(self, task, model=None, max_length=512):\n",
    "        self.pipe = pipeline(task, model=model)\n",
    "        self.max_length = max_length\n",
    "        self.task = task\n",
    "    \n",
    "    def __call__(self, text, **kwargs):\n",
    "        # Handle empty input\n",
    "        if not text or not text.strip():\n",
    "            return {'error': 'Empty input', 'result': None}\n",
    "        \n",
    "        # Handle very long text\n",
    "        if len(text) > self.max_length * 4:  # Rough char estimate\n",
    "            text = text[:self.max_length * 4]\n",
    "            truncated = True\n",
    "        else:\n",
    "            truncated = False\n",
    "        \n",
    "        try:\n",
    "            result = self.pipe(text, **kwargs)\n",
    "            return {\n",
    "                'result': result,\n",
    "                'truncated': truncated,\n",
    "                'error': None\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'result': None,\n",
    "                'truncated': truncated\n",
    "            }\n",
    "\n",
    "# Usage\n",
    "robust = RobustPipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Test with various inputs\n",
    "test_cases = [\n",
    "    \"Normal text that works fine.\",\n",
    "    \"\",  # Empty\n",
    "    \"   \",  # Whitespace only\n",
    "    \"A \" * 10000  # Very long\n",
    "]\n",
    "\n",
    "for i, text in enumerate(test_cases):\n",
    "    result = robust(text)\n",
    "    print(f\"Test {i+1}: error={result['error']}, truncated={result.get('truncated', False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "1. **Custom Pipelines**: Inherit from `Pipeline` or existing pipeline classes\n",
    "2. **Optimization**: FP16, quantization, and ONNX can significantly improve performance\n",
    "3. **Batching**: Always batch when processing multiple inputs\n",
    "4. **Local Models**: Save/load for offline or production use\n",
    "5. **Chaining**: Combine pipelines for complex workflows\n",
    "6. **Error Handling**: Always wrap pipelines for production use\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to [05_production_patterns.ipynb](05_production_patterns.ipynb) for production best practices!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
