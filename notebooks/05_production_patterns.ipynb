{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Production Patterns\n",
    "\n",
    "Best practices for using Hugging Face Pipelines in production:\n",
    "\n",
    "1. Model Loading Strategies\n",
    "2. Caching and Performance\n",
    "3. API Design Patterns\n",
    "4. Monitoring and Logging\n",
    "5. Scaling Considerations\n",
    "6. Security Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import time\n",
    "import logging\n",
    "from functools import lru_cache\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Model Loading Strategies\n",
    "\n",
    "### 1.1 Singleton Pattern for Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    \"\"\"\n",
    "    Singleton pattern to ensure models are loaded only once.\n",
    "    \"\"\"\n",
    "    _instance = None\n",
    "    _pipelines = {}\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def get_pipeline(self, task: str, model: str = None, **kwargs) -> pipeline:\n",
    "        \"\"\"\n",
    "        Get or create a pipeline. Cached for reuse.\n",
    "        \"\"\"\n",
    "        key = f\"{task}_{model or 'default'}\"\n",
    "        \n",
    "        if key not in self._pipelines:\n",
    "            print(f\"Loading pipeline: {key}\")\n",
    "            if model:\n",
    "                self._pipelines[key] = pipeline(task, model=model, **kwargs)\n",
    "            else:\n",
    "                self._pipelines[key] = pipeline(task, **kwargs)\n",
    "        \n",
    "        return self._pipelines[key]\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear all cached pipelines.\"\"\"\n",
    "        self._pipelines.clear()\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Usage\n",
    "manager = ModelManager()\n",
    "\n",
    "# First call loads the model\n",
    "pipe1 = manager.get_pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Second call returns cached model\n",
    "pipe2 = manager.get_pipeline(\"sentiment-analysis\")\n",
    "\n",
    "print(f\"Same instance: {pipe1 is pipe2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Lazy Loading with Context Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyPipeline:\n",
    "    \"\"\"\n",
    "    Lazy-loaded pipeline that initializes on first use.\n",
    "    \"\"\"\n",
    "    def __init__(self, task: str, model: str = None, **kwargs):\n",
    "        self.task = task\n",
    "        self.model = model\n",
    "        self.kwargs = kwargs\n",
    "        self._pipeline = None\n",
    "    \n",
    "    @property\n",
    "    def pipe(self):\n",
    "        if self._pipeline is None:\n",
    "            print(f\"Initializing {self.task} pipeline...\")\n",
    "            self._pipeline = pipeline(\n",
    "                self.task, \n",
    "                model=self.model, \n",
    "                **self.kwargs\n",
    "            )\n",
    "        return self._pipeline\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.pipe(*args, **kwargs)\n",
    "    \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        self.unload()\n",
    "    \n",
    "    def unload(self):\n",
    "        \"\"\"Explicitly unload the model.\"\"\"\n",
    "        if self._pipeline is not None:\n",
    "            del self._pipeline\n",
    "            self._pipeline = None\n",
    "            import gc\n",
    "            gc.collect()\n",
    "\n",
    "# Usage - model loads only when first called\n",
    "lazy_classifier = LazyPipeline(\"sentiment-analysis\")\n",
    "print(\"Pipeline created (not loaded yet)\")\n",
    "\n",
    "# First call triggers loading\n",
    "result = lazy_classifier(\"This loads the model!\")\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Caching and Performance\n",
    "\n",
    "### 2.1 Result Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from collections import OrderedDict\n",
    "\n",
    "class CachedPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline with LRU result caching.\n",
    "    \"\"\"\n",
    "    def __init__(self, task: str, model: str = None, cache_size: int = 1000):\n",
    "        self.pipe = pipeline(task, model=model)\n",
    "        self.cache = OrderedDict()\n",
    "        self.cache_size = cache_size\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def _get_cache_key(self, text: str) -> str:\n",
    "        \"\"\"Generate cache key from input.\"\"\"\n",
    "        return hashlib.md5(text.encode()).hexdigest()\n",
    "    \n",
    "    def __call__(self, text: str, use_cache: bool = True):\n",
    "        if use_cache:\n",
    "            key = self._get_cache_key(text)\n",
    "            \n",
    "            if key in self.cache:\n",
    "                self.hits += 1\n",
    "                # Move to end (most recently used)\n",
    "                self.cache.move_to_end(key)\n",
    "                return self.cache[key]\n",
    "            \n",
    "            self.misses += 1\n",
    "            result = self.pipe(text)\n",
    "            \n",
    "            # Add to cache\n",
    "            self.cache[key] = result\n",
    "            \n",
    "            # Evict oldest if over capacity\n",
    "            while len(self.cache) > self.cache_size:\n",
    "                self.cache.popitem(last=False)\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            return self.pipe(text)\n",
    "    \n",
    "    @property\n",
    "    def hit_rate(self) -> float:\n",
    "        total = self.hits + self.misses\n",
    "        return self.hits / total if total > 0 else 0.0\n",
    "\n",
    "# Usage\n",
    "cached = CachedPipeline(\"sentiment-analysis\", cache_size=100)\n",
    "\n",
    "# First call - cache miss\n",
    "result1 = cached(\"I love this product!\")\n",
    "print(f\"Call 1 - Hit rate: {cached.hit_rate:.2%}\")\n",
    "\n",
    "# Second call with same input - cache hit\n",
    "result2 = cached(\"I love this product!\")\n",
    "print(f\"Call 2 - Hit rate: {cached.hit_rate:.2%}\")\n",
    "\n",
    "# Different input - cache miss\n",
    "result3 = cached(\"This is terrible!\")\n",
    "print(f\"Call 3 - Hit rate: {cached.hit_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Request Batching Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from queue import Queue\n",
    "import uuid\n",
    "\n",
    "class BatchingService:\n",
    "    \"\"\"\n",
    "    Accumulate requests and process in batches.\n",
    "    \"\"\"\n",
    "    def __init__(self, task: str, batch_size: int = 16, timeout: float = 0.1):\n",
    "        self.pipe = pipeline(task)\n",
    "        self.batch_size = batch_size\n",
    "        self.timeout = timeout\n",
    "        self.request_queue = Queue()\n",
    "        self.results = {}\n",
    "        self._stop = False\n",
    "        \n",
    "        # Start worker thread\n",
    "        self.worker = threading.Thread(target=self._process_batches, daemon=True)\n",
    "        self.worker.start()\n",
    "    \n",
    "    def _process_batches(self):\n",
    "        \"\"\"Background worker to process accumulated requests.\"\"\"\n",
    "        while not self._stop:\n",
    "            batch = []\n",
    "            ids = []\n",
    "            \n",
    "            # Collect requests\n",
    "            try:\n",
    "                while len(batch) < self.batch_size:\n",
    "                    req_id, text = self.request_queue.get(timeout=self.timeout)\n",
    "                    batch.append(text)\n",
    "                    ids.append(req_id)\n",
    "            except:\n",
    "                pass  # Timeout reached\n",
    "            \n",
    "            # Process batch\n",
    "            if batch:\n",
    "                results = self.pipe(batch)\n",
    "                for req_id, result in zip(ids, results):\n",
    "                    self.results[req_id] = result\n",
    "    \n",
    "    def predict(self, text: str, timeout: float = 5.0):\n",
    "        \"\"\"Submit request and wait for result.\"\"\"\n",
    "        req_id = str(uuid.uuid4())\n",
    "        self.request_queue.put((req_id, text))\n",
    "        \n",
    "        # Wait for result\n",
    "        start = time.time()\n",
    "        while req_id not in self.results:\n",
    "            if time.time() - start > timeout:\n",
    "                raise TimeoutError(\"Request timed out\")\n",
    "            time.sleep(0.01)\n",
    "        \n",
    "        result = self.results.pop(req_id)\n",
    "        return result\n",
    "    \n",
    "    def stop(self):\n",
    "        self._stop = True\n",
    "\n",
    "# Usage\n",
    "service = BatchingService(\"sentiment-analysis\", batch_size=8)\n",
    "\n",
    "# Simulate concurrent requests\n",
    "results = []\n",
    "for text in [\"Great!\", \"Terrible!\", \"Okay.\"]:\n",
    "    result = service.predict(text)\n",
    "    results.append(result)\n",
    "    print(f\"{text}: {result[0]['label']}\")\n",
    "\n",
    "service.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. API Design Patterns\n",
    "\n",
    "### 3.1 Service Class Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ClassificationResult:\n",
    "    \"\"\"Structured result for classification.\"\"\"\n",
    "    text: str\n",
    "    label: str\n",
    "    confidence: float\n",
    "    processing_time_ms: float\n",
    "    model_name: str\n",
    "\n",
    "class TextClassificationService:\n",
    "    \"\"\"\n",
    "    Production-ready text classification service.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
    "        self.model_name = model_name\n",
    "        self.pipe = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def classify(self, text: str) -> ClassificationResult:\n",
    "        \"\"\"\n",
    "        Classify a single text.\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # Validate input\n",
    "        if not text or not text.strip():\n",
    "            raise ValueError(\"Input text cannot be empty\")\n",
    "        \n",
    "        # Run inference\n",
    "        result = self.pipe(text)[0]\n",
    "        processing_time = (time.time() - start) * 1000\n",
    "        \n",
    "        return ClassificationResult(\n",
    "            text=text,\n",
    "            label=result['label'],\n",
    "            confidence=result['score'],\n",
    "            processing_time_ms=processing_time,\n",
    "            model_name=self.model_name\n",
    "        )\n",
    "    \n",
    "    def classify_batch(self, texts: List[str]) -> List[ClassificationResult]:\n",
    "        \"\"\"\n",
    "        Classify multiple texts efficiently.\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # Validate\n",
    "        texts = [t for t in texts if t and t.strip()]\n",
    "        if not texts:\n",
    "            return []\n",
    "        \n",
    "        # Batch inference\n",
    "        results = self.pipe(texts)\n",
    "        processing_time = (time.time() - start) * 1000 / len(texts)\n",
    "        \n",
    "        return [\n",
    "            ClassificationResult(\n",
    "                text=text,\n",
    "                label=result['label'],\n",
    "                confidence=result['score'],\n",
    "                processing_time_ms=processing_time,\n",
    "                model_name=self.model_name\n",
    "            )\n",
    "            for text, result in zip(texts, results)\n",
    "        ]\n",
    "\n",
    "# Usage\n",
    "service = TextClassificationService()\n",
    "\n",
    "# Single classification\n",
    "result = service.classify(\"This product is amazing!\")\n",
    "print(f\"Single: {result.label} ({result.confidence:.2%}) in {result.processing_time_ms:.1f}ms\")\n",
    "\n",
    "# Batch classification\n",
    "batch_results = service.classify_batch([\n",
    "    \"Great product!\",\n",
    "    \"Terrible experience.\",\n",
    "    \"It's okay.\"\n",
    "])\n",
    "for r in batch_results:\n",
    "    print(f\"Batch: {r.text[:20]}... -> {r.label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Monitoring and Logging\n",
    "\n",
    "### 4.1 Instrumented Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class InstrumentedPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline with comprehensive monitoring.\n",
    "    \"\"\"\n",
    "    def __init__(self, task: str, model: str = None):\n",
    "        self.pipe = pipeline(task, model=model)\n",
    "        self.logger = logging.getLogger(f\"pipeline.{task}\")\n",
    "        \n",
    "        # Metrics\n",
    "        self.total_requests = 0\n",
    "        self.total_latency_ms = 0\n",
    "        self.errors = 0\n",
    "        self.label_counts = {}\n",
    "    \n",
    "    def __call__(self, text: str, **kwargs):\n",
    "        self.total_requests += 1\n",
    "        request_id = f\"{datetime.now().timestamp():.0f}\"\n",
    "        \n",
    "        self.logger.info(f\"[{request_id}] Processing request, length={len(text)}\")\n",
    "        start = time.time()\n",
    "        \n",
    "        try:\n",
    "            result = self.pipe(text, **kwargs)\n",
    "            latency = (time.time() - start) * 1000\n",
    "            self.total_latency_ms += latency\n",
    "            \n",
    "            # Track label distribution\n",
    "            label = result[0]['label']\n",
    "            self.label_counts[label] = self.label_counts.get(label, 0) + 1\n",
    "            \n",
    "            self.logger.info(\n",
    "                f\"[{request_id}] Completed: label={label}, \"\n",
    "                f\"confidence={result[0]['score']:.3f}, \"\n",
    "                f\"latency={latency:.1f}ms\"\n",
    "            )\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.errors += 1\n",
    "            self.logger.error(f\"[{request_id}] Error: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get current metrics.\"\"\"\n",
    "        return {\n",
    "            'total_requests': self.total_requests,\n",
    "            'errors': self.errors,\n",
    "            'error_rate': self.errors / max(self.total_requests, 1),\n",
    "            'avg_latency_ms': self.total_latency_ms / max(self.total_requests, 1),\n",
    "            'label_distribution': self.label_counts\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "monitored = InstrumentedPipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Process some requests\n",
    "texts = [\"Great!\", \"Terrible!\", \"Amazing!\", \"Awful!\"]\n",
    "for text in texts:\n",
    "    monitored(text)\n",
    "\n",
    "# View metrics\n",
    "metrics = monitored.get_metrics()\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Total requests: {metrics['total_requests']}\")\n",
    "print(f\"  Avg latency: {metrics['avg_latency_ms']:.1f}ms\")\n",
    "print(f\"  Label distribution: {metrics['label_distribution']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. FastAPI Integration Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example FastAPI application structure\n",
    "\n",
    "fastapi_code = '''\n",
    "# app.py\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import pipeline\n",
    "from typing import List\n",
    "import time\n",
    "\n",
    "app = FastAPI(title=\"Text Classification API\")\n",
    "\n",
    "# Load model at startup\n",
    "classifier = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_model():\n",
    "    global classifier\n",
    "    classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "class TextInput(BaseModel):\n",
    "    text: str\n",
    "\n",
    "class BatchInput(BaseModel):\n",
    "    texts: List[str]\n",
    "\n",
    "class ClassificationResponse(BaseModel):\n",
    "    label: str\n",
    "    confidence: float\n",
    "    processing_time_ms: float\n",
    "\n",
    "@app.post(\"/classify\", response_model=ClassificationResponse)\n",
    "async def classify(input: TextInput):\n",
    "    if not input.text.strip():\n",
    "        raise HTTPException(400, \"Text cannot be empty\")\n",
    "    \n",
    "    start = time.time()\n",
    "    result = classifier(input.text)[0]\n",
    "    latency = (time.time() - start) * 1000\n",
    "    \n",
    "    return ClassificationResponse(\n",
    "        label=result[\"label\"],\n",
    "        confidence=result[\"score\"],\n",
    "        processing_time_ms=latency\n",
    "    )\n",
    "\n",
    "@app.post(\"/classify/batch\")\n",
    "async def classify_batch(input: BatchInput):\n",
    "    texts = [t for t in input.texts if t.strip()]\n",
    "    if not texts:\n",
    "        return []\n",
    "    \n",
    "    results = classifier(texts)\n",
    "    return [\n",
    "        {\"text\": text, \"label\": r[\"label\"], \"confidence\": r[\"score\"]}\n",
    "        for text, r in zip(texts, results)\n",
    "    ]\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\", \"model_loaded\": classifier is not None}\n",
    "\n",
    "# Run with: uvicorn app:app --host 0.0.0.0 --port 8000\n",
    "'''\n",
    "\n",
    "print(\"FastAPI Example:\")\n",
    "print(fastapi_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Security Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecurePipeline:\n",
    "    \"\"\"\n",
    "    Pipeline with security considerations.\n",
    "    \"\"\"\n",
    "    def __init__(self, task: str, max_length: int = 1000):\n",
    "        self.pipe = pipeline(task)\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def sanitize_input(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Sanitize and validate input.\n",
    "        \"\"\"\n",
    "        # Remove null bytes\n",
    "        text = text.replace('\\x00', '')\n",
    "        \n",
    "        # Limit length\n",
    "        if len(text) > self.max_length:\n",
    "            text = text[:self.max_length]\n",
    "        \n",
    "        # Strip excessive whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def __call__(self, text: str, **kwargs):\n",
    "        # Validate input type\n",
    "        if not isinstance(text, str):\n",
    "            raise TypeError(\"Input must be a string\")\n",
    "        \n",
    "        # Sanitize\n",
    "        clean_text = self.sanitize_input(text)\n",
    "        \n",
    "        if not clean_text:\n",
    "            raise ValueError(\"Input text is empty after sanitization\")\n",
    "        \n",
    "        return self.pipe(clean_text, **kwargs)\n",
    "\n",
    "# Usage\n",
    "secure = SecurePipeline(\"sentiment-analysis\", max_length=500)\n",
    "\n",
    "# Normal input\n",
    "result = secure(\"This is a normal input.\")\n",
    "print(f\"Normal: {result}\")\n",
    "\n",
    "# Very long input (gets truncated)\n",
    "long_text = \"word \" * 1000\n",
    "result = secure(long_text)\n",
    "print(f\"Long input handled: {result[0]['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Production Checklist\n",
    "\n",
    "### Before Deployment\n",
    "- [ ] Model loaded at startup (not per-request)\n",
    "- [ ] Input validation implemented\n",
    "- [ ] Error handling in place\n",
    "- [ ] Logging configured\n",
    "- [ ] Health check endpoint available\n",
    "- [ ] Batch processing for throughput\n",
    "\n",
    "### Performance\n",
    "- [ ] GPU enabled if available\n",
    "- [ ] FP16 or quantization considered\n",
    "- [ ] Batch size optimized\n",
    "- [ ] Result caching implemented\n",
    "- [ ] Connection pooling configured\n",
    "\n",
    "### Security\n",
    "- [ ] Input sanitization\n",
    "- [ ] Rate limiting\n",
    "- [ ] Input length limits\n",
    "- [ ] Authentication (if needed)\n",
    "\n",
    "### Monitoring\n",
    "- [ ] Latency metrics\n",
    "- [ ] Error rates\n",
    "- [ ] Model drift detection\n",
    "- [ ] Resource utilization\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the Hugging Face Pipelines learning guide. You now know:\n",
    "\n",
    "1. **Basics**: What pipelines are and how to use them\n",
    "2. **NLP**: All major NLP pipeline types\n",
    "3. **Multimodal**: Vision, audio, and cross-modal pipelines\n",
    "4. **Advanced**: Custom pipelines and optimization\n",
    "5. **Production**: Best practices for deployment\n",
    "\n",
    "Happy building! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
