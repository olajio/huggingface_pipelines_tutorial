{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - NLP Pipelines Deep Dive\n",
    "\n",
    "This notebook covers all major NLP pipeline types:\n",
    "- Text Classification (sentiment, topic, etc.)\n",
    "- Token Classification (NER, POS tagging)\n",
    "- Question Answering\n",
    "- Text Generation\n",
    "- Summarization\n",
    "- Translation\n",
    "- Fill-Mask\n",
    "- Zero-Shot Classification\n",
    "- Feature Extraction (Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Text Classification\n",
    "\n",
    "Classify entire text into predefined categories.\n",
    "\n",
    "**Use Cases:**\n",
    "- Sentiment analysis\n",
    "- Spam detection\n",
    "- Topic categorization\n",
    "- Intent detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "texts = [\n",
    "    \"I absolutely love this new feature!\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"It's okay, nothing special.\"\n",
    "]\n",
    "\n",
    "results = sentiment(texts)\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"{result['label']:8} ({result['score']:.2f}): {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class Classification (5-star ratings)\n",
    "rating_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    ")\n",
    "\n",
    "reviews = [\n",
    "    \"Excellent product! Exceeded all expectations.\",\n",
    "    \"It works but could be better.\",\n",
    "    \"Terrible quality, complete waste of money.\"\n",
    "]\n",
    "\n",
    "for review in reviews:\n",
    "    result = rating_classifier(review)[0]\n",
    "    stars = result['label']  # e.g., \"5 stars\"\n",
    "    print(f\"{stars}: {review[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top-k predictions\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Return all class scores\n",
    "result = classifier(\"This movie was pretty good but had some issues.\", top_k=None)\n",
    "print(\"All class scores:\")\n",
    "for item in result:\n",
    "    print(f\"  {item['label']}: {item['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Token Classification (NER, POS)\n",
    "\n",
    "Classify each token in a sequence.\n",
    "\n",
    "**Use Cases:**\n",
    "- Named Entity Recognition (NER)\n",
    "- Part-of-Speech tagging\n",
    "- Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition\n",
    "ner = pipeline(\"ner\", grouped_entities=True)  # grouped_entities merges multi-token entities\n",
    "\n",
    "text = \"Elon Musk founded SpaceX in Hawthorne, California. Tesla's headquarters moved to Austin, Texas in 2021.\"\n",
    "\n",
    "entities = ner(text)\n",
    "print(\"Entities found:\")\n",
    "for entity in entities:\n",
    "    print(f\"  {entity['entity_group']:12} | {entity['word']:20} | Score: {entity['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without grouping - see individual tokens\n",
    "ner_ungrouped = pipeline(\"ner\", grouped_entities=False)\n",
    "\n",
    "text = \"Barack Obama was born in Hawaii.\"\n",
    "entities = ner_ungrouped(text)\n",
    "\n",
    "print(\"Token-level entities:\")\n",
    "for entity in entities:\n",
    "    print(f\"  {entity['entity']:12} | {entity['word']:15} | Score: {entity['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity types explained:\n",
    "# B-PER = Beginning of Person name\n",
    "# I-PER = Inside/Continuation of Person name\n",
    "# B-ORG = Beginning of Organization\n",
    "# B-LOC = Beginning of Location\n",
    "# B-MISC = Miscellaneous entity\n",
    "\n",
    "# Real-world example: Extract entities for analysis\n",
    "news = \"\"\"\n",
    "Microsoft announced today that CEO Satya Nadella will visit the European Union \n",
    "headquarters in Brussels next week to discuss AI regulations with Margrethe Vestager.\n",
    "\"\"\"\n",
    "\n",
    "entities = ner(news)\n",
    "print(\"\\nExtracted from news article:\")\n",
    "for e in entities:\n",
    "    print(f\"  [{e['entity_group']}] {e['word']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Question Answering\n",
    "\n",
    "Extract answers from a given context.\n",
    "\n",
    "**Use Cases:**\n",
    "- Customer support bots\n",
    "- Document search\n",
    "- Knowledge extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = pipeline(\"question-answering\")\n",
    "\n",
    "context = \"\"\"\n",
    "Hugging Face was founded in 2016 by ClÃ©ment Delangue, Julien Chaumond, and Thomas Wolf.\n",
    "The company is headquartered in New York City. They created the Transformers library,\n",
    "which has become the most popular library for natural language processing. The library\n",
    "supports PyTorch, TensorFlow, and JAX frameworks. As of 2024, the Hugging Face Hub\n",
    "hosts over 500,000 models.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"When was Hugging Face founded?\",\n",
    "    \"Who founded Hugging Face?\",\n",
    "    \"Where is the company headquartered?\",\n",
    "    \"How many models are on the Hub?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa(question=question, context=context)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {result['answer']} (confidence: {result['score']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get multiple possible answers\n",
    "result = qa(\n",
    "    question=\"Who founded the company?\",\n",
    "    context=context,\n",
    "    top_k=3  # Get top 3 answers\n",
    ")\n",
    "\n",
    "print(\"Top 3 possible answers:\")\n",
    "for i, ans in enumerate(result, 1):\n",
    "    print(f\"{i}. {ans['answer']} (score: {ans['score']:.3f}, position: {ans['start']}-{ans['end']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Text Generation\n",
    "\n",
    "Generate text continuations.\n",
    "\n",
    "**Use Cases:**\n",
    "- Chatbots\n",
    "- Creative writing\n",
    "- Code completion\n",
    "- Auto-complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "prompt = \"The key to successful machine learning is\"\n",
    "\n",
    "# Basic generation\n",
    "result = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(\"Generated text:\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation with different sampling strategies\n",
    "\n",
    "# 1. Greedy (deterministic)\n",
    "result_greedy = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=30,\n",
    "    do_sample=False  # Greedy decoding\n",
    ")\n",
    "print(\"Greedy:\")\n",
    "print(result_greedy[0]['generated_text'])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. With temperature (controls randomness)\n",
    "# Lower = more focused, Higher = more creative/random\n",
    "result_temp = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=30,\n",
    "    do_sample=True,\n",
    "    temperature=0.9  # Higher = more random\n",
    ")\n",
    "print(\"Temperature=0.9 (creative):\")\n",
    "print(result_temp[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Top-k sampling\n",
    "result_topk = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=30,\n",
    "    do_sample=True,\n",
    "    top_k=50  # Only sample from top 50 tokens\n",
    ")\n",
    "print(\"Top-k=50:\")\n",
    "print(result_topk[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Nucleus (top-p) sampling\n",
    "result_topp = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=30,\n",
    "    do_sample=True,\n",
    "    top_p=0.92  # Sample from tokens that make up 92% of probability mass\n",
    ")\n",
    "print(\"Top-p=0.92 (nucleus):\")\n",
    "print(result_topp[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Multiple sequences with beam search\n",
    "result_beam = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=30,\n",
    "    num_beams=5,  # Beam search with 5 beams\n",
    "    num_return_sequences=3,\n",
    "    no_repeat_ngram_size=2  # Prevent repetition\n",
    ")\n",
    "print(\"Beam search (3 sequences):\")\n",
    "for i, seq in enumerate(result_beam, 1):\n",
    "    print(f\"{i}. {seq['generated_text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Summarization\n",
    "\n",
    "Condense long text into shorter summaries.\n",
    "\n",
    "**Use Cases:**\n",
    "- News summarization\n",
    "- Document TL;DR\n",
    "- Meeting notes\n",
    "- Email summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "article = \"\"\"\n",
    "Artificial intelligence has made remarkable strides in recent years, transforming \n",
    "industries from healthcare to finance. Machine learning models can now diagnose \n",
    "diseases with accuracy rivaling human doctors, while natural language processing \n",
    "enables chatbots to handle customer service inquiries with increasing sophistication.\n",
    "\n",
    "However, these advances come with significant challenges. Concerns about AI bias, \n",
    "job displacement, and the potential misuse of AI-generated content have led to \n",
    "calls for stricter regulation. The European Union has proposed the AI Act, which \n",
    "would establish strict rules for high-risk AI applications.\n",
    "\n",
    "Despite these challenges, investment in AI continues to grow. Tech giants and \n",
    "startups alike are racing to develop more powerful and efficient AI systems. \n",
    "The development of large language models like GPT-4 and Claude has demonstrated \n",
    "the potential for AI systems that can engage in complex reasoning and creative tasks.\n",
    "\n",
    "Looking ahead, experts predict that AI will become increasingly integrated into \n",
    "everyday life, from autonomous vehicles to personalized education. The key \n",
    "challenge will be ensuring that these powerful tools are developed and deployed \n",
    "responsibly, with appropriate safeguards to protect privacy and prevent misuse.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarizer(article, max_length=100, min_length=30, do_sample=False)\n",
    "print(\"Summary:\")\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control summary length\n",
    "short_summary = summarizer(article, max_length=50, min_length=20)\n",
    "print(\"Short summary:\")\n",
    "print(short_summary[0]['summary_text'])\n",
    "print()\n",
    "\n",
    "long_summary = summarizer(article, max_length=150, min_length=80)\n",
    "print(\"Longer summary:\")\n",
    "print(long_summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Translation\n",
    "\n",
    "Convert text between languages.\n",
    "\n",
    "**Use Cases:**\n",
    "- Content localization\n",
    "- Real-time translation\n",
    "- Multi-language support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English to French\n",
    "translator_en_fr = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "\n",
    "text = \"Machine learning is transforming the way we interact with technology.\"\n",
    "result = translator_en_fr(text)\n",
    "print(f\"English: {text}\")\n",
    "print(f\"French:  {result[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English to German\n",
    "translator_en_de = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\")\n",
    "\n",
    "result = translator_en_de(text)\n",
    "print(f\"German:  {result[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English to Spanish\n",
    "translator_en_es = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "\n",
    "result = translator_en_es(text)\n",
    "print(f\"Spanish: {result[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Fill-Mask (Masked Language Modeling)\n",
    "\n",
    "Predict masked tokens in text.\n",
    "\n",
    "**Use Cases:**\n",
    "- Autocomplete\n",
    "- Text correction\n",
    "- Understanding model knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "# Note: Use [MASK] token for BERT-based models\n",
    "text = \"Paris is the [MASK] of France.\"\n",
    "\n",
    "predictions = fill_mask(text)\n",
    "print(f\"Input: {text}\")\n",
    "print(\"\\nPredictions:\")\n",
    "for pred in predictions:\n",
    "    print(f\"  {pred['token_str']:15} (score: {pred['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa uses <mask> token\n",
    "fill_mask_roberta = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
    "\n",
    "text = \"The <mask> jumped over the lazy dog.\"\n",
    "predictions = fill_mask_roberta(text)\n",
    "\n",
    "print(f\"Input: {text}\")\n",
    "print(\"\\nPredictions:\")\n",
    "for pred in predictions[:5]:\n",
    "    print(f\"  {pred['token_str']:15} (score: {pred['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple masks\n",
    "text = \"The [MASK] [MASK] is a large language model.\"\n",
    "# Note: Some models don't support multiple masks well\n",
    "# For multiple masks, consider using autoregressive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Zero-Shot Classification\n",
    "\n",
    "Classify text into categories without training on those specific categories.\n",
    "\n",
    "**Use Cases:**\n",
    "- Flexible categorization\n",
    "- Rapid prototyping\n",
    "- When you don't have labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "text = \"The new MacBook Pro features an M3 chip with improved performance and battery life.\"\n",
    "candidate_labels = [\"technology\", \"sports\", \"politics\", \"entertainment\", \"business\"]\n",
    "\n",
    "result = zero_shot(text, candidate_labels)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(\"\\nClassification:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"  {label:15} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-label classification\n",
    "text = \"The company announced record profits while also laying off 10% of its workforce.\"\n",
    "labels = [\"business\", \"employment\", \"finance\", \"positive news\", \"negative news\"]\n",
    "\n",
    "result = zero_shot(text, labels, multi_label=True)  # Allow multiple labels\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(\"\\nMulti-label classification:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    marker = \"âœ“\" if score > 0.5 else \" \"\n",
    "    print(f\"  {marker} {label:15} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom use case: Support ticket routing\n",
    "tickets = [\n",
    "    \"I can't log into my account, password reset isn't working.\",\n",
    "    \"When will my order arrive? It's been 2 weeks.\",\n",
    "    \"I want a refund, the product is damaged.\",\n",
    "    \"How do I upgrade my subscription plan?\"\n",
    "]\n",
    "\n",
    "categories = [\"login issues\", \"shipping\", \"refunds\", \"billing\", \"product issues\"]\n",
    "\n",
    "print(\"Support Ticket Router:\\n\")\n",
    "for ticket in tickets:\n",
    "    result = zero_shot(ticket, categories)\n",
    "    print(f\"Ticket: {ticket[:50]}...\")\n",
    "    print(f\"Route to: {result['labels'][0]} ({result['scores'][0]:.2f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Feature Extraction (Embeddings)\n",
    "\n",
    "Get vector representations (embeddings) of text.\n",
    "\n",
    "**Use Cases:**\n",
    "- Semantic search\n",
    "- Clustering\n",
    "- Similarity comparison\n",
    "- RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = pipeline(\"feature-extraction\", model=\"distilbert-base-uncased\")\n",
    "\n",
    "text = \"Machine learning is a subset of artificial intelligence.\"\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = feature_extractor(text)\n",
    "\n",
    "import numpy as np\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"  - Batch size: {embeddings.shape[0]}\")\n",
    "print(f\"  - Sequence length: {embeddings.shape[1]}\")\n",
    "print(f\"  - Hidden dimension: {embeddings.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentence embedding by mean pooling\n",
    "def get_sentence_embedding(text, extractor):\n",
    "    features = extractor(text)\n",
    "    # Mean pool across sequence dimension\n",
    "    embedding = np.array(features).mean(axis=1).squeeze()\n",
    "    return embedding\n",
    "\n",
    "sentence_embedding = get_sentence_embedding(text, feature_extractor)\n",
    "print(f\"Sentence embedding shape: {sentence_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity between sentences\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "sentences = [\n",
    "    \"I love machine learning.\",\n",
    "    \"AI is fascinating.\",\n",
    "    \"The weather is nice today.\",\n",
    "    \"Deep learning is powerful.\"\n",
    "]\n",
    "\n",
    "embeddings = [get_sentence_embedding(s, feature_extractor) for s in sentences]\n",
    "\n",
    "# Compare first sentence to all others\n",
    "print(f\"Comparing: '{sentences[0]}'\\n\")\n",
    "for i, sent in enumerate(sentences[1:], 1):\n",
    "    sim = cosine_similarity(embeddings[0], embeddings[i])\n",
    "    print(f\"  vs '{sent}': {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Text-to-Text Generation (T5, FLAN-T5)\n",
    "\n",
    "General-purpose text transformation.\n",
    "\n",
    "**Use Cases:**\n",
    "- Paraphrasing\n",
    "- Grammar correction\n",
    "- Style transfer\n",
    "- Multi-task NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5 uses task prefixes\n",
    "t5 = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "# Summarization\n",
    "result = t5(\"summarize: The quick brown fox jumps over the lazy dog. The dog was sleeping in the sun.\")\n",
    "print(f\"Summary: {result[0]['generated_text']}\")\n",
    "\n",
    "# Translation\n",
    "result = t5(\"translate English to German: Hello, how are you?\")\n",
    "print(f\"Translation: {result[0]['generated_text']}\")\n",
    "\n",
    "# Question answering\n",
    "result = t5(\"answer: What is the capital of France?\")\n",
    "print(f\"Answer: {result[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Pipeline Selection Guide\n",
    "\n",
    "| I want to... | Use this pipeline | Recommended model |\n",
    "|--------------|-------------------|-------------------|\n",
    "| Analyze sentiment | `sentiment-analysis` | distilbert-base-uncased-finetuned-sst-2-english |\n",
    "| Extract entities | `ner` | dslim/bert-base-NER |\n",
    "| Answer questions | `question-answering` | deepset/roberta-base-squad2 |\n",
    "| Generate text | `text-generation` | gpt2, meta-llama/Llama-2-7b |\n",
    "| Summarize text | `summarization` | facebook/bart-large-cnn |\n",
    "| Translate | `translation_xx_to_yy` | Helsinki-NLP/opus-mt-* |\n",
    "| Classify flexibly | `zero-shot-classification` | facebook/bart-large-mnli |\n",
    "| Get embeddings | `feature-extraction` | sentence-transformers/* |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to [03_multimodal_pipelines.ipynb](03_multimodal_pipelines.ipynb) for vision and audio pipelines!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
