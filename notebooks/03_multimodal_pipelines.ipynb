{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Multimodal Pipelines\n",
    "\n",
    "This notebook covers pipelines for non-text modalities:\n",
    "\n",
    "**Computer Vision:**\n",
    "- Image Classification\n",
    "- Object Detection\n",
    "- Image Segmentation\n",
    "- Depth Estimation\n",
    "\n",
    "**Audio:**\n",
    "- Automatic Speech Recognition (ASR)\n",
    "- Audio Classification\n",
    "- Text-to-Speech\n",
    "\n",
    "**Multimodal:**\n",
    "- Image-to-Text (Captioning)\n",
    "- Visual Question Answering\n",
    "- Document Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional dependencies for multimodal\n",
    "# !pip install transformers torch torchvision torchaudio\n",
    "# !pip install pillow soundfile librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load images from URL\n",
    "def load_image_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "# Sample image URLs for testing\n",
    "CAT_IMAGE = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\"\n",
    "DOG_IMAGE = \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/YellowLabradorLooking_new.jpg/1200px-YellowLabradorLooking_new.jpg\"\n",
    "STREET_IMAGE = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/A_modern_city_street.jpg/1280px-A_modern_city_street.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Computer Vision Pipelines\n",
    "\n",
    "### 1.1 Image Classification\n",
    "\n",
    "Classify entire images into categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image classification pipeline\n",
    "image_classifier = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Load a sample image\n",
    "image = load_image_from_url(CAT_IMAGE)\n",
    "\n",
    "# Classify\n",
    "results = image_classifier(image)\n",
    "\n",
    "print(\"Image Classification Results:\")\n",
    "for result in results[:5]:\n",
    "    print(f\"  {result['label']:30} ({result['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also pass image paths or URLs directly\n",
    "results = image_classifier(CAT_IMAGE)  # URL\n",
    "print(\"Classification from URL:\", results[0]['label'])\n",
    "\n",
    "# Or local file path\n",
    "# results = image_classifier(\"./my_image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch classification\n",
    "images = [CAT_IMAGE, DOG_IMAGE]\n",
    "batch_results = image_classifier(images)\n",
    "\n",
    "for img_url, result in zip(images, batch_results):\n",
    "    print(f\"Image: {result[0]['label']} ({result[0]['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Object Detection\n",
    "\n",
    "Detect and locate objects within images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object detection pipeline\n",
    "object_detector = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n",
    "\n",
    "# Detect objects\n",
    "image = load_image_from_url(STREET_IMAGE)\n",
    "detections = object_detector(image)\n",
    "\n",
    "print(f\"Found {len(detections)} objects:\")\n",
    "for det in detections:\n",
    "    print(f\"  {det['label']:15} (score: {det['score']:.3f})\")\n",
    "    print(f\"    Box: {det['box']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by confidence threshold\n",
    "threshold = 0.9\n",
    "confident_detections = [d for d in detections if d['score'] > threshold]\n",
    "\n",
    "print(f\"\\nHigh-confidence detections (>{threshold}):\")\n",
    "for det in confident_detections:\n",
    "    print(f\"  {det['label']}: {det['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Image Segmentation\n",
    "\n",
    "Segment images at pixel level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic segmentation\n",
    "segmenter = pipeline(\"image-segmentation\", model=\"facebook/detr-resnet-50-panoptic\")\n",
    "\n",
    "image = load_image_from_url(STREET_IMAGE)\n",
    "segments = segmenter(image)\n",
    "\n",
    "print(f\"Found {len(segments)} segments:\")\n",
    "for seg in segments:\n",
    "    print(f\"  {seg['label']:20} (score: {seg.get('score', 'N/A')})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Depth Estimation\n",
    "\n",
    "Estimate depth from single images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth estimation pipeline\n",
    "depth_estimator = pipeline(\"depth-estimation\", model=\"Intel/dpt-large\")\n",
    "\n",
    "image = load_image_from_url(STREET_IMAGE)\n",
    "result = depth_estimator(image)\n",
    "\n",
    "print(f\"Depth map shape: {result['depth'].size}\")\n",
    "print(f\"Predicted depth type: {type(result['predicted_depth'])}\")\n",
    "\n",
    "# The result contains:\n",
    "# - 'depth': PIL Image with depth visualization\n",
    "# - 'predicted_depth': Raw depth tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Audio Pipelines\n",
    "\n",
    "### 2.1 Automatic Speech Recognition (ASR)\n",
    "\n",
    "Convert speech to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASR pipeline\n",
    "asr = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\")\n",
    "\n",
    "# You can pass:\n",
    "# - Audio file path: asr(\"audio.mp3\")\n",
    "# - Audio URL: asr(\"https://example.com/audio.mp3\")\n",
    "# - NumPy array with sample rate\n",
    "\n",
    "# Example with a sample audio URL\n",
    "AUDIO_SAMPLE = \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\"\n",
    "\n",
    "result = asr(AUDIO_SAMPLE)\n",
    "print(f\"Transcription: {result['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASR with timestamps\n",
    "result = asr(AUDIO_SAMPLE, return_timestamps=True)\n",
    "\n",
    "print(\"Transcription with timestamps:\")\n",
    "if 'chunks' in result:\n",
    "    for chunk in result['chunks']:\n",
    "        start, end = chunk['timestamp']\n",
    "        print(f\"  [{start:.2f}s - {end:.2f}s]: {chunk['text']}\")\n",
    "else:\n",
    "    print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long-form transcription (for audio > 30 seconds)\n",
    "long_asr = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-base\",\n",
    "    chunk_length_s=30,  # Process in 30-second chunks\n",
    "    stride_length_s=5   # Overlap between chunks\n",
    ")\n",
    "\n",
    "# This handles long audio files automatically\n",
    "# result = long_asr(\"long_audio.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Audio Classification\n",
    "\n",
    "Classify audio into categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio classification pipeline\n",
    "audio_classifier = pipeline(\n",
    "    \"audio-classification\",\n",
    "    model=\"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    ")\n",
    "\n",
    "# Classify audio\n",
    "# result = audio_classifier(\"audio_sample.wav\")\n",
    "\n",
    "# The model can detect:\n",
    "# - Speech, music, environmental sounds\n",
    "# - Specific instruments\n",
    "# - Animal sounds\n",
    "# - etc.\n",
    "\n",
    "print(\"Audio classification can detect various sound categories.\")\n",
    "print(\"Pass an audio file path to classify.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Multimodal Pipelines\n",
    "\n",
    "### 3.1 Image-to-Text (Image Captioning)\n",
    "\n",
    "Generate text descriptions of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image captioning pipeline\n",
    "captioner = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Generate caption\n",
    "image = load_image_from_url(CAT_IMAGE)\n",
    "result = captioner(image)\n",
    "\n",
    "print(f\"Caption: {result[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple images\n",
    "images = [CAT_IMAGE, DOG_IMAGE]\n",
    "results = captioner(images)\n",
    "\n",
    "for img, result in zip(['Cat', 'Dog'], results):\n",
    "    print(f\"{img}: {result[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visual Question Answering (VQA)\n",
    "\n",
    "Answer questions about images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQA pipeline\n",
    "vqa = pipeline(\"visual-question-answering\", model=\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "image = load_image_from_url(CAT_IMAGE)\n",
    "\n",
    "# Ask questions about the image\n",
    "questions = [\n",
    "    \"What animal is in the image?\",\n",
    "    \"What color is the animal?\",\n",
    "    \"Is the animal sleeping?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = vqa(image=image, question=question)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {result[0]['answer']} (score: {result[0]['score']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Document Question Answering\n",
    "\n",
    "Extract information from document images (invoices, forms, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document QA pipeline\n",
    "doc_qa = pipeline(\"document-question-answering\", model=\"impira/layoutlm-document-qa\")\n",
    "\n",
    "# This works with document images (PDFs rendered as images, scanned documents, etc.)\n",
    "# Particularly useful for:\n",
    "# - Invoice processing\n",
    "# - Form extraction\n",
    "# - Receipt parsing\n",
    "\n",
    "# Example usage (with a document image):\n",
    "# result = doc_qa(\n",
    "#     image=\"invoice.png\",\n",
    "#     question=\"What is the total amount?\"\n",
    "# )\n",
    "\n",
    "print(\"Document QA pipeline loaded.\")\n",
    "print(\"Pass a document image and question to extract information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Zero-Shot Image Classification\n",
    "\n",
    "Classify images into custom categories without training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot image classification (CLIP)\n",
    "zero_shot_image = pipeline(\n",
    "    \"zero-shot-image-classification\",\n",
    "    model=\"openai/clip-vit-base-patch32\"\n",
    ")\n",
    "\n",
    "image = load_image_from_url(CAT_IMAGE)\n",
    "\n",
    "# Define custom categories\n",
    "candidate_labels = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a bird\", \"a photo of a fish\"]\n",
    "\n",
    "result = zero_shot_image(image, candidate_labels=candidate_labels)\n",
    "\n",
    "print(\"Zero-shot classification:\")\n",
    "for item in result:\n",
    "    print(f\"  {item['label']:25} ({item['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Multimodal Pipeline Reference\n",
    "\n",
    "| Pipeline | Input | Output | Model Examples |\n",
    "|----------|-------|--------|----------------|\n",
    "| `image-classification` | Image | Labels + scores | google/vit-base-patch16-224 |\n",
    "| `object-detection` | Image | Boxes + labels | facebook/detr-resnet-50 |\n",
    "| `image-segmentation` | Image | Masks + labels | facebook/detr-resnet-50-panoptic |\n",
    "| `depth-estimation` | Image | Depth map | Intel/dpt-large |\n",
    "| `automatic-speech-recognition` | Audio | Text | openai/whisper-base |\n",
    "| `audio-classification` | Audio | Labels | MIT/ast-finetuned-audioset |\n",
    "| `image-to-text` | Image | Caption | Salesforce/blip-image-captioning-base |\n",
    "| `visual-question-answering` | Image + Question | Answer | dandelin/vilt-b32-finetuned-vqa |\n",
    "| `document-question-answering` | Doc image + Question | Answer | impira/layoutlm-document-qa |\n",
    "| `zero-shot-image-classification` | Image + Labels | Scores | openai/clip-vit-base-patch32 |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to [04_advanced_pipelines.ipynb](04_advanced_pipelines.ipynb) for custom pipelines and optimization!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
